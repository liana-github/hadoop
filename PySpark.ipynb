{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DataFrames** in PySpark and pandas are both tabular data structures, but they belong to different ecosystems and have some differences:\n",
    "\n",
    "**1. PySpark DataFrame:**\n",
    "- Distributed Computing: PySpark DataFrames are distributed across a cluster, enabling processing of large-scale data using multiple machines.\n",
    "- Resilient Distributed Datasets (RDDs): Underlying PySpark DataFrames, RDDs allow fault-tolerant parallel processing.\n",
    "- Lazy Evaluation: PySpark uses lazy evaluation, postponing the execution of transformations until an action is performed.\n",
    "- Scale: Ideal for handling big data, scaling well with large datasets that exceed the memory capacity of a single machine.\n",
    "\n",
    "**2. pandas DataFrame:**\n",
    "- Single-machine: pandas DataFrames operate on a single machine's memory.\n",
    "- Rich Functionality: pandas provides extensive data manipulation and analysis tools, making it great for data cleaning, exploration, and small to medium-sized datasets.\n",
    "- Integration with Python Libraries: pandas integrates well with other Python libraries, simplifying workflows and analysis.\n",
    "- Eager Evaluation: pandas executes operations immediately, which can be beneficial for smaller datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Choosing between them depends on the specific use case**\n",
    "\n",
    "- PySpark DataFrame is preferable for handling distributed and large-scale data when working with clusters and huge datasets that won't fit into memory on a single machine.\n",
    "- pandas DataFrame is excellent for smaller to medium-sized datasets, offering a rich set of tools and straightforward syntax for data manipulation and analysis on a single machine.\n",
    "\n",
    "In summary, PySpark DataFrames are suitable for big data processing and distributed computing, whereas pandas DataFrames are well-suited for small to medium-sized data analysis and manipulation on a single machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"ReadCSV\").getOrCreate()\n",
    "\n",
    "# Read CSV file into a DataFrame\n",
    "file_path = \"/project/statesPopulation.csv\"\n",
    "df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "\n",
    "# Show the DataFrame schema and some sample data\n",
    "df.printSchema()\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using sort\n",
    "df.sort(df.State.desc()).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using sum and groupBy\n",
    "df.groupBy('State').sum('Population').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using sum function and alias\n",
    "from pyspark.sql.functions import sum\n",
    "df.groupBy('State', 'Year').agg(sum('Population').alias(\"Nb\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying pivot operation\n",
    "pivot_df = df.groupBy(\"State\").pivot(\"Year\").agg({\"Population\": \"sum\"})\n",
    "\n",
    "# Displaying the pivoted DataFrame\n",
    "pivot_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In PySpark, you can leverage SQL queries to manipulate DataFrames using the spark.sql() method\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"SQLExample\").getOrCreate()\n",
    "\n",
    "# Read a CSV file into a DataFrame\n",
    "file_path = \"/project/statesPopulation.csv\"\n",
    "\n",
    "df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "\n",
    "# Create a temporary view of the DataFrame\n",
    "df.createOrReplaceTempView(\"my_table\")\n",
    "\n",
    "# Run SQL queries on the DataFrame\n",
    "result = spark.sql(\"SELECT * FROM my_table where State like '%aba%'\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run SQL queries on the DataFrame\n",
    "query = \"\"\"\n",
    "            SELECT State, SUM(Population) AS Nb\n",
    "            FROM my_table\n",
    "            GROUP BY State\n",
    "            ORDER BY State\n",
    "        \"\"\"\n",
    "result = spark.sql(query)\n",
    "result.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
